{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this project I attempt to make a classification model to predict the odds of making individual NFL field goals, taking into account factors that could be known at the time of the attempt, especially the player attempting the kick. Field goals are important plays during an NFL game, and games can at times come down to a single attempt. Because many factors can influence the outcome of a field goal attempt, and because the success of the attempt can be so important to the outcome of games, being able to understand how likely a try is to succeed or fail can be very important information. In particular, the skill of the kicker attempting the field goal can be of vital importance. In other work on this topic, the skill of the individual kicker is often ignored in favor of a more general model. In addition, other work tends to focus more on classification accuracy across a large period of time, and less on the odds of any one specific kick being made or missed. This is one of the main differences between other work I have seen and the model I am trying to make. In order to make a model that is able to generate the odds of a specific kick being made or missed, that also takes into account individual kicker skill, the most important metric is the f1 score. This is because in order to produce meaningful odds on any one kick being made or missed, the model has to handle missed field goals correctly. The f1 score combines precision and recall, giving a good metric for measuring how well the model performs on predicting misses. Classification accuracy might be less important, but it is still important, so I keep track of it. I take predictors I believe to be meaningful, and use them to train logistic regression, decision tree, and random forest models, that I score in both f1 score and classification accuracy using 10-fold cross validation. The best performing model is an oversampled logistic regression model, using RandomOverSampler, with a miss/made ratio of .6. The model has a cross validation f1 score average of .425, and an average classification accuracy of .768. When used on the testing data, the results drop to .363 for f1 score, and .745 for classification accuracy. Both of these results leave much to be desired, and there is much room for doubt as to how good this model actually is. However, it does serve as a good starting point for future work, as it does achieve the two major goals I had with the project. It is able to predict probability of make or miss for individual field goals (even if not the most accurately), and it takes the kicker into account strongly when making the predictions. In the future, I hope to improve this model greatly by finding ways to improve the f1 score, possibly by finding predictors I am not currently using, or interaction effects. I also want to make it possible to make predictions on kicks with qualitative predictor values that were not present in the training data, although there is potential for this to affect the quality of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the most important and unpredictable plays in the NFL is the field goal attempt. While worth less than a touchdown, the field goal can be attempted from range, and it is not uncommon for games to be decided on a single field goal attempt. There are many factors that can affect whether a try is made or missed, such as the skill of the players, the length of the kick, and the weather conditions. The number of influential factors adds a decent amount of unpredictability to a field goal attempt, and when combined with the potential importance riding on a single attempt, this play is one that receives a great deal of attention. Being able to predict how likely an attempt is to be made or missed is thus not only interesting as a spectator in understanding the game state, but important for teams when deciding whether or not to attempt the field goal at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My motivation for this project is a combination of interest in the sport, interest in kicking plays more specifically, and wanting a way to understand coaching decisions when a team decides whether or not to attempt a field goal. Since every team has a different kicker, the skill of the kicker is an important factor in making these evaluations. There are several different ways one could approach modeling field goal attempts. For this reason, it is important to determine what the specific goals of the model are. In this case, I am more interested in creating a model that is able to give an accurate representation of the odds of success of a specific, individual kick than creating a model that has the highest success rate across many field goals. In addition, while there are many factors that influence the success rate of field goal attempts, the one I am most interested in is the skill of the individual kicker. For these reasons, the two most important things I look for in the model are the f1 score, and whether or not the model is able to make kicker specific predictions. The f1 score will represent how well the model does at handling missed attempts, which is important because the model needs to do a good job at predicting misses to be able to give accurate odds of each individual kick. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There has been plenty of work around the topic of field goal accuracy. Often, though, the focus is to determine whether a team should attempt a field goal, go for it on 4th down, or punt the ball, and the chances of making a field goal are only used to calculate the “correct” decision given the situation. In other cases, field goal accuracy is the focal point, but sometimes factors such as the kick speed and launch angle are used. Since I am more concerned with the odds of making a kick at the time of the snap than at the time of the kick, it would not be right to use these types of predictors. Also, in most cases, predictive models are generalized, meaning they do not account for the skill of the individual kicker, and at times, they also do not account for weather conditions or the stadium. Sometimes, models look to rate kickers, to see how individual kickers perform in general, not on how the player making the kick changes the chances of that specific kick being made or missed. All of this is to say, most work on this topic is concerned with what decision to make in the moment, or the odds of making a field goal of a certain distance in general. This is very different from my goal, which is to make a model that is able to predict the odds of making individual kicks, especially taking into account the kickers themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to make this model, I combine the play by play data with the weather data for all field goals attempted between 2014 and 2018. I start with 2014 because the NFL raised the height of the goal uprights from 30 feet to 35 feet prior to the start of the 2014 season. While this change might not make a significant difference in model accuracy, I decide to only use data from seasons with the 35 foot goal uprights for the sake of consistency. The columns that I have selected from the play by play data are the team attempting the field goal (posteam), the number of seconds remaining in the half (half_seconds_remaining), the half (game_half), the distance of the kick (kick_distance), the amount the team attempting the field goal is leading by (score_differential), and the player attempting the kick (kicker_player_name). From the weather data, I have selected the stadium the game is being played in (stadium), the temperature at kickoff (weather_temperature), the wind speed at kickoff (weather_wind_mph), and any import weather information at kickoff (weather_detail). With this data, I train multiple classification models and compare their performance across 10-fold cross validation before ultimately selecting the best model. Again, the best model is determined by the model with the best balance between f1 score and classification accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before I begin making models, I split the data into training and testing data at an 80/20 split. I then perform One Hot Encoding on the data to handle the quantitative predictors, and encode the target from “made” and “missed” to “0” and “1”. I then scale the data.\n",
    "## For the linear regression models, I begin by creating a simple model, with no penalty. When performing cross validation, the model performs poorly in terms of f1 score, getting an average score of .24. The results vary fairly significantly across each test as well. In order to attempt to improve the model, I then use a grid search to try to maximize the f1 score. The best performing model had a C value of 10, penalty of ‘l2’, and ended up performing almost identically to the simple no penalty model, with an average f1 score of .24, and an average standard deviation of .07. A final experiment for the logistic regression models is using RandomOverSampler to over sample the data, with the thinking being that this would make the model more likely to predict a miss, which could improve the f1 score. After cross validation, the best model, determined by best balance between f1 and accuracy, had an average f1 score of .425 and an average classification accuracy of .768. While those scores are far from ideal, the f1 score is significantly higher than the previous models.\n",
    "## For the decision tree models, I again begin by making a simple model. The f1 score is 0, as the model is simply not predicting a single miss. Using a grid search to maximize the f1 score produces a model with an average score of .27. This is significantly lower than the best performing logistic regression model, so the next experiment is once again over sampling to try to improve f1 score. The results of .406 for f1 and .76 for accuracy are not bad, but still worse than the best performing logistic regression model.\n",
    "## For the random forest models, I start with a simple model. The results are very poor, so I use a grid search to try to maximize f1 score. The best model only has an f1 score of .19, so I again turn to over sampling to try to increase the f1 score. The f1 score of .401 is not as high as the best logistic regression model, nor is the classification accuracy of .745. For this reason, I select the oversampled logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The resulting model has an average f1 score of  .425 and an average classification accuracy of .768 across cross validation. On the testing data, the model performs a bit worth across both metrics, with an f1 score of .363 and a classification accuracy of .745. Objectively, neither of these metrics are all that remarkable. The classification accuracy is less than simply predicting every field goal as a make. If raw accuracy was the goal, this would be more alarming. However, since the goal is to be able to predict the percentage chance of each individual field goal, the model needs to be able to predict misses. While it would be preferable to have a model that had a higher classification accuracy than predicting make in every case and still be able to predict misses, this model is still better than predicting make in all cases for the purposes of the project. That is in no way saying the model is great, however. The more concerning metric is the low f1 score. For this assignment, the f1 score indicates how well the model does when predicting a miss. Since the model needs to be able to predict misses to have a realistic chance of presenting the odds of making each individual kick, this metric is vitally important. The cross validation score of .425 leaves much to be desired in this regard, and the model performs even worse on the testing data, although that could just be due to variation related to the specific testing data used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In conclusion, this model has much room for improvement. The low accuracy score can be downplayed, as the overall classification accuracy is not the most important metric. However, even given this, ideally the classification accuracy would be significantly higher. As mentioned above, the bigger concern is the f1 score. Ideally, this score would be much, much higher, as that would indicate that the model was successful when making miss predictions. The model is able to predict misses, which means it is capable of giving the odds of making each individual kick, and it definitely takes the individual kicker into account, both of which were the main goals of this project. However, the low f1 score means that the model does not do the best job of handling miss predictions, which means there is plenty of room to doubt the accuracy of the odds it predicts for each individual kick. In conclusion, I would say this model is successful as a starting point for building a better model in the future, as it does everything I wanted it to do. However, it does not seem to do these things exceptionally well, so I would say it is a failure as a good model in its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and Later Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In addition to the issues discussed above, there are some important limitations with this model. The most glaring issue is partially due to lack of available data, which is that it can only account for weather details at the time of kickoff. It is always possible for weather conditions to change during the course of the game, such as a strong wind dying down or starting up. Ideally, the weather information at the time of the kick would be used, but that information is not easily available. Another limitation of this model is that it is currently unable to make predictions if it encounters a qualitive predictor it has not encountered. For example, if a new kicker enters the league, or a new stadium opens, the model will be unable to make predictions until data for the kicker or stadium is used in training. \n",
    "## When looking to the future, the most important thing is to find a way to improve the f1 score. At present, my theory is that the predictors simply do not do a good enough job accounting for the differences between made and missed field goals. Adding some predictors is an option where data allows, but I expect that there might be some interaction effects between some of the predictors, so this might be a good place to start. As for the limitations, other than continuing to look for available data sources, there is not much to be done for the weather data. For making predictions on new values for quantitative predictors, one possible solution would be to add a value for each that represented an unknown value, and direct the model to translate any unknown values to this additional column. However, this could also affect the quality of the model, so how well this approach will work remains to be seen. Other than that, I will hopefully be able to keep adding in yearly data as it becomes easily available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Contributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play by play data: https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016?select=NFL+Play+by+Play+2009-2018+%28v5%29.csv\n",
    "## Weather data: https://www.kaggle.com/tobycrabtree/nfl-scores-and-betting-data!\n",
    "## NFL raising height of field goal uprights: https://www.espn.com/nfl/story/_/id/10676450/nfl-goalpost-uprights-increase-35-feet\n",
    "## Example of a model rating kickers by comparing actual field goal percentage with expected field goal percentage: https://medium.com/the-spax/modeling-the-nfl-field-goal-8e3c3fff0c3b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
